from text_module import normalize, tokenize, count_freq, top_n 

def run_tests():
    print("=== normalize ===")
    print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"), "‚Üí", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")
    print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"), "‚Üí", "–µ–∂–∏–∫, –µ–ª–∫–∞")
    print(normalize("Hello\r\nWorld"), "‚Üí", "hello world")
    print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "), "‚Üí", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã")

    print("\n=== tokenize ===")
    print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"), "‚Üí", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"])
    print(tokenize("hello,world!!!"), "‚Üí", ["hello", "world"])
    print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"), "‚Üí", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"])
    print(tokenize("2025 –≥–æ–¥"), "‚Üí", ["2025", "–≥–æ–¥"])
    print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"), "‚Üí", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"])

    print("\n=== count_freq + top_n ===")
    tokens1 = ["a", "b", "a", "c", "b", "a"]
    freq1 = count_freq(tokens1)
    print(freq1, "‚Üí", {"a": 3, "b": 2, "c": 1})
    print(top_n(freq1, n=2), "‚Üí", [("a", 3), ("b", 2)])

    tokens2 = ["bb", "aa", "bb", "aa", "cc"]
    freq2 = count_freq(tokens2)
    print(freq2, "‚Üí", {"aa": 2, "bb": 2, "cc": 1})
    print(sorted(freq2.items(), key=lambda x: (-x[1], x[0]))[:2], "‚Üí", [("aa", 2), ("bb", 2)])


if __name__ == "__main__":
    run_tests()